As part of this exercise, user data from an E-commerce website and their click and buy events and occurrences were analysed.

The datasets were downloaded from the given links, and uploaded to HDFS and then used as arguments to run the Hadoop cluster according to requirements. 

In the first part of the exercise, the items that were clicked most during the month of April were found. For this, each line of data was first split into two using the "-" delimiter/regex, after which, from the odd lines (containing the second part of the split) were taken and their first two characters were checked ("04" - for the month of April) and if they matched the string, the substring from 20th to 29th position was taken, which gives the ItemID (identifier for a particular item). The occurrences and frequencies of these were found and then grouped. A Hash function was used to sort the values in descending order and a counter used to get the top 10 occurrences. The ItemID and frequency were obtained as output for the top 10 values. 

For the second part of the exercise, where the total revenue generated for each block was required, each line was first split using a comma "," delimiter, and the third and fourth arguments of each line were multiplied to give the revenue. The time parameter was taken as sub-strings by using indices of the T and Z characters and this was again split using the ":" regex . The first argument of this gives the hour parameter which was taken as the Key, the revenue for each hour being the value. This was sorted in descending order by revenue using a hash function. The values were printed as Hour (\tab) Revenue.

The third part involved taking input from both click.txt and buy.txt. Split each line by <,> delimiter. if the length of split is 5, its coming from buy or else click. In mapper I write ItemId as key and type as value (1 for click and 2 for buy). In reducer, I check the number of times 1 and 2 appeared for a particular id. Take the success rate by dividing the parameters and put it in map<itemid , successrate> . Sort the map by value in the cleaup part of reducer  and write the top 10 elements into the output

Three sub-directories - ItemClick, TimeBlocks and SuccessRate were created in the PartC directory and the output of each of the tasks were uploaded. A Java sub-directory was created in each sub-directory in which the jar and source files were uploaded along with the shell script required to run the Hadoop cluster for generating the obtained outputs. 
